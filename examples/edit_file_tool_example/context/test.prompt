# Test Generation Guidelines

### General Setup
- The generated test file will be for a module located in the `edit_file_tool/` directory. The test file should be written as if it is located in the `tests/` directory, and the module under test should be imported using its full path, for example: `import edit_file_tool.config as config`.
- **CRITICAL PROJECT STRUCTURE REQUIREMENTS:**
  - Test files will be located at `tests/test_module.py` (where "module" is the name of the component being tested)
  - The corresponding source code will be located at `edit_file_tool/module.py`
  - **IMPORT STATEMENTS**: Use absolute imports from the edit_file_tool package: `from edit_file_tool.module import function_name` or `import edit_file_tool.module as module`
  - **EXECUTION CONTEXT**: Tests should be designed to run from the project root directory (PDD_user_experience_test_manual)
  - **MODULE DISCOVERY**: Tests must work when run from the project root using `pytest tests/test_module.py` or `python -m pytest tests/test_module.py`
- All tests must be written using the `pytest` framework.
- Follow the existing convention of creating a "Test Plan" in a docstring or comment at the top of the test file, outlining the rationale and methods for each test case.
- The generated test code must ONLY use ASCII characters (no unicode symbols, emojis, checkmarks, or special characters) to avoid encoding errors during execution on different systems.
- **EXECUTION INSTRUCTIONS**: Include a comment block at the top of the test file explaining how to run it:
  ```python
  # To run this test:
  # Option 1: From project root: pytest tests/test_module.py
  # Option 2: From project root: python -m pytest tests/test_module.py
  # Option 3: From tests directory: pytest test_module.py
  # Option 4: Set PYTHONPATH: set PYTHONPATH=%PYTHONPATH%;C:\path\to\project
  ```

### Module Discovery and Dependency Validation
- **CRITICAL**: Before generating any test that imports from `edit_file_tool`, you MUST first examine the actual project structure to understand what modules exist.
- **Discovery Process**: 
  1. List the contents of the `edit_file_tool/` directory to see available modules
  2. Read the target module file to understand its actual imports and dependencies
  3. Verify that all imported dependencies actually exist in the project
- **Import Validation**: If the module under test imports other components (e.g., `from .other_module import SomeClass`), verify that `edit_file_tool/other_module.py` exists before writing tests that mock `SomeClass`
- **Fallback Strategy**: If a dependency doesn't exist as expected:
  1. Check if the functionality exists in a different module (e.g., `utils.py`)
  2. Adapt the test to mock the actual implementation pattern (functions vs classes)
  3. Document the discrepancy in test comments
- **Example Discovery Pattern**:
  ```python
  # Before writing tests for edit_file_tool.claude_api:
  # 1. Check: ls edit_file_tool/ to see what files exist
  # 2. Read: edit_file_tool/claude_api.py to see what it imports
  # 3. Verify: If it imports from .cost_calculator, check if cost_calculator.py exists
  # 4. Adapt: If cost_calculator.py doesn't exist, find where cost functions actually live
  ```

### Asynchronous Testing
- For testing any asynchronous functions (i.e., functions defined with `async def`), the `pytest-asyncio` library must be used. Test functions for asynchronous code must be decorated with `@pytest.mark.asyncio`.
- **Fixtures**: Only define a pytest fixture with `async def` if the setup code *within the fixture itself* needs to use `await`. If a fixture simply creates an object that has `async` methods, the fixture can and should remain synchronous (`def`). However, if the mocked object *instance itself* is directly awaited in the application code, the mock instance returned by the class must be an `AsyncMock` to be awaitable.

### Testing Strategy
- **Mocking Strategy**: 
  - **File Operations**: Use real temporary files via pytest's `tmp_path` fixture rather than mocking filesystem operations. This ensures path validation and file I/O work correctly together.
  - **External Dependencies**: Mock only true external dependencies (API calls, network requests, third-party services).
  - **Error Simulation**: For file operation errors (permissions, not found), use real filesystem conditions or platform-specific techniques rather than mocks.
  - **Validation Functions**: Avoid mocking core validation logic (`validate_file_path`, etc.) as this disconnects tests from actual behavior.
- **Failure Modes**: When testing functions that can fail, use `pytest.raises` to verify that the correct custom exceptions are thrown.
- **Data-Driven Tests**: Use `pytest.mark.parametrize` to create data-driven tests that cover not only the "happy path" but also common edge cases. This includes empty inputs (`""`), zero values, negative numbers, out-of-bounds indices, and varied data formats (e.g., different line endings like `\n` vs. `\r\n`).

### Testing Stateful Asynchronous Orchestrators

When testing a function that orchestrates multiple asynchronous calls in a loop (an "orchestrator"), special care must be taken with mocks, especially when stateful objects like a `conversation_history` list are passed as arguments.

-   **The State Snapshot Problem**: A mock stores a *reference* to mutable arguments (like lists or dictionaries), not a *copy*. If the application modifies that list over several loop iterations, inspecting the mock's call arguments after the function finishes will only show you the **final state** of the list for *every* call. This can make it impossible to verify the state at intermediate steps.
-   **Recommended Solution: The Snapshotting Side Effect**: The most robust way to solve this is to capture a snapshot of the arguments at the moment of each call. Use the mock's `side_effect` attribute to assign a helper function that takes a copy of the arguments and stores them before returning the desired mock response.

    ```python
    # In your test:
    history_snapshots = []
    def snapshot_side_effect(*args, **kwargs):
        history_snapshots.append(kwargs['messages'].copy()) # Or deepcopy
        return (mock_response, mock_cost_data)

    mock_call_api.side_effect = snapshot_side_effect
    await orchestrator_function()

    # Now you can assert the state of the first call, second, etc.
    assert len(history_snapshots[0]) == 1
    assert len(history_snapshots[1]) == 3
    ```
-   **Targeted Assertions**: This snapshotting technique allows you to precisely target assertions. For example, you can verify the contents of the `tool_result` message that was sent on the second call, rather than making fragile assumptions about its position in the final history.

### Logging Philosophy

A consistent logging strategy is crucial for debugging and monitoring. Generated tests should enforce the following logging standards:

-   **`DEBUG` for Intent and Detail**: `DEBUG`-level logs should be used to signal the *start* of an operation (e.g., "Attempting to read file...") and to output variable contents or verbose details that are useful for deep debugging.
-   **`INFO` for Successful Completion**: `INFO`-level logs should confirm the successful completion of a significant, user-facing action (e.g., "File written successfully.").
-   **`WARNING` for Handled Issues**: `WARNING`-level logs should be used for recoverable or non-critical problems that the application handles gracefully. For example, if an operation's goal is to ensure a file is deleted and the file is already gone, a `WARNING` is appropriate because it's not an error, but it's worth noting.
-   **`ERROR` for Unhandled Failures**: `ERROR`-level logs must be used when an operation fails and the application cannot recover, typically just before raising an exception.

### File-Based Testing Best Practices
- **Real Files vs. Mocks**: When testing file editing tools, prefer real temporary files over filesystem mocks. Use pytest's `tmp_path` fixture to create isolated test directories.
- **Path Validation**: Never mock path validation functions directly. Instead, create real file structures that trigger the validation paths you want to test.
- **Error Conditions**: Simulate file errors using real conditions:
  - Non-existent files: Simply don't create the file
  - Permission errors: Use `os.chmod()` on temporary files
  - Directory vs. file confusion: Create actual directories when testing directory handling
- **Cleanup**: Rely on pytest's automatic temporary directory cleanup rather than manual cleanup in teardown methods.

### Assertions and Logging
- **Error Messages**: When asserting error messages, it is often best to check for key substrings (`assert 'important part' in error_message`) instead of matching the exact string. This prevents tests from failing due to minor, non-functional changes in wording. However, when an error message is part of a structured response to another system (like an error payload in a JSON object for an LLM), it's better to be more precise. Consider asserting against the exact string generated by the application (e.g., `assert result['error'] == str(MyException(msg))`) to ensure the 'contract' with the external system is met.
- **Logging**: Use the `pytest` `caplog` fixture to capture and verify log output. **Tests should be written to enforce the standards outlined in the "Logging Philosophy" section.** Assertions must check both the log level (`record.levelname`) and the message content. If a test needs to check for `DEBUG`-level logs, it must first set the capture level at the beginning of the test using `caplog.set_level(logging.DEBUG)`. 