# Module: edit_file_tool/claude_api.py

## Context
<required_context_files>
  % Architecture defines role, ownership, and dependency ordering.
  <architecture>
    <include>pdd/architecture.json</include>
  </architecture>

  % Shared utilities (logging, validation, cache decisions, cost formatting, config).
  <utils>
    <include>edit_file_tool/utils.py</include>
  </utils>

  % The README grounds UX, CLI, caching, and cost-tracking requirements.
  <readme>
    <include>EDIT_FILE_TOOL_README.md</include>
  </readme>

  % NOTE: The following dependent modules may not exist yet during initial generation,
  % but their expected import patterns and usage are documented below for interface design.
  % If these files exist, include them for exact interface validation.
  <cli_dependency>
    <include>edit_file_tool/cli.py</include>
  </cli_dependency>

  <core_dependency>
    <include>edit_file_tool/core.py</include>
  </core_dependency>
</required_context_files>

## Module Dependencies and Usage
This module is imported by other modules in the system. The expected import patterns are:

1. **CLI Module** (`edit_file_tool/cli.py`):
   ```python
   from edit_file_tool.claude_api import DEFAULT_MODEL, SUPPORTED_MODELS
   ```
   - Uses `DEFAULT_MODEL` for fallback when user doesn't specify a model
   - Uses `SUPPORTED_MODELS` to validate user-provided model names

2. **Core Module** (`edit_file_tool/core.py`):
   ```python
   from edit_file_tool.claude_api import MODEL_TO_TEXT_EDITOR_TOOL, call_claude_api
   ```
   - Uses `call_claude_api` as the main API interface for making Claude requests
   - Uses `MODEL_TO_TEXT_EDITOR_TOOL` to determine valid text editor tool names for routing tool calls

## Required Module Exports
The module MUST export these items for backward compatibility with dependent modules:

- `DEFAULT_MODEL` (str): Default Claude model identifier
- `SUPPORTED_MODELS` (Set/Dict/Collection): Collection of supported model names for validation
- `MODEL_TO_TEXT_EDITOR_TOOL` (Dict): Mapping of models to their text editor tool configurations
- `call_claude_api` (function): Main asynchronous API calling function
- `CostInfo` (TypedDict): Type definition for cost information structure

## Role and Responsibilities
This module serves as the central service layer for all interactions with the Anthropic Claude API. Its primary responsibility is to abstract the complexity of API calls into a single, robust, and easy-to-use asynchronous function. It will manage the entire lifecycle of an API request, from payload construction to response parsing, including advanced features like prompt caching, cost calculation, and tool integration.

Key responsibilities include:
- Encapsulating `anthropic` SDK calls to provide a simplified interface.
- Constructing valid API request payloads, including messages, system prompts, and tool definitions.
- Implementing logic for Anthropic's prompt caching to reduce costs and latency.
- Implementing internal cost calculation logic to provide accurate cost breakdowns for each API call, accounting for standard and cached token usage.
- Handling API-related errors gracefully, including rate limits, timeouts, and other connection issues, with a built-in retry mechanism.
- Supporting the appropriate `text_editor` tool version (based on the model being used) and `think_tool` by correctly formatting them in requests and parsing their usage from responses.

## Requirements

### Model and API Integration
1.  **Models:** Support the following Claude models with their corresponding text editor tool versions as documented in <anthropic_text_editor_docs><web>https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/text-editor-tool</web></anthropic_text_editor_docs>:
    - Claude 4 Opus & Sonnet: `text_editor_20250728`
    - Claude Sonnet 3.7: `text_editor_20250124` 
    - Claude Sonnet 3.5: `text_editor_20241022`
    - The primary model should be `claude-3-7-sonnet-20250219` as a configurable default
    - Implement model validation to ensure only supported models are used
2.  **Tools:** The implementation must dynamically select the appropriate text editor tool version based on the model being used, plus support for the think tool for reasoning traces.
3.  **API Client:** Use the official `anthropic` Python SDK for all API interactions.
4.  **Asynchronous Operations:** All API calls must be asynchronous using `async/await` and the `anthropic.AsyncAnthropic` client.

### Functional Requirements
1.  **API Key Management:** Use `python-dotenv` to call `load_dotenv(override=False)` to load a `.env` file if present, then securely read `ANTHROPIC_API_KEY` from the environment. Raise a `ValueError` if the key is not found.
2.  **Prompt Caching:**
    - Implement logic to construct a cache-optimized request when `use_cache` is `True`.
    - This involves separating the file content (the large, cacheable part of the prompt) from the user instructions (the variable part).
    - The file content message block must include `cache_control={'type': 'ephemeral'}`.
    - Refer to Anthropic's documentation for prompt caching best practices. <anthropic_prompt_caching_docs><web>https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching</web></anthropic_prompt_caching_docs>
3.  **Cost Calculation:**
    - After each API call, extract token usage details from the `response.usage` object, including `input_tokens`, `output_tokens`, `cache_creation_input_tokens`, and `cache_read_input_tokens`.
    - **CRITICAL:** According to Anthropic's API documentation, these token values follow this relationship:
      - `input_tokens`: Base/standard input tokens (NOT total tokens)
      - `cache_creation_input_tokens`: Additional tokens used for cache creation
      - `cache_read_input_tokens`: Additional tokens read from existing cache
      - Total tokens processed = `input_tokens + cache_creation_input_tokens + cache_read_input_tokens`
    - **Do NOT subtract cache tokens from input_tokens** - this is mathematically incorrect and will cause negative values.
    - Implement cost calculation logic using current Anthropic pricing as detailed in <anthropic_model_overview><web>https://docs.anthropic.com/en/docs/about-claude/models/overview</web></anthropic_model_overview> (per million tokens):
      - Claude Opus 4.1/4: Input $15, Output $75
      - Claude Sonnet 4/3.7/3.5: Input $3, Output $15  
      - Claude Haiku 3.5: Input $0.80, Output $4
    - Cache pricing: Cache writes are charged at input token rates, cache reads are typically 10% of input token rates.
    - **Cost formula:** `(input_tokens * input_rate) + (output_tokens * output_rate) + (cache_creation_tokens * input_rate) + (cache_read_tokens * cache_read_rate)`
4.  **Error Handling and Resilience:**
    - Implement robust error handling for common API exceptions (`anthropic.APIError`, `anthropic.RateLimitError`, `anthropic.APIConnectionError`).
    - Implement a retry mechanism with exponential backoff for transient errors like rate limiting and connection issues.
5.  **Think Tool Support:** The response handling should be able to identify and extract content from `<thinking>` tags if the `think_tool` was used by the model.

% Here is an example of how to define and use the think_tool for API calls: <think_tool_definition_example><include>examples/think_tool_example.py</include></think_tool_definition_example>

### Security Requirements
1.  The Anthropic API key must never be hardcoded or logged.
2.  Input validation should be considered for parameters passed to the main function, although the primary validation will occur in higher-level modules.

### Documentation Requirements
1.  All public functions and classes must have comprehensive Google-style docstrings.
2.  Full type hinting must be used for all function signatures and variables, utilizing the `typing` module.

## Dependencies

### Standard Library:
- `asyncio`: For asynchronous operations.
- `logging`: For logging errors and debug information.
- `typing`: For type hints (`List`, `Dict`, `Tuple`, `Any`, `Optional`, `TypedDict`).
- `json`: For handling tool schemas if needed.

### Third-party:
- `anthropic`: The official Python SDK for the Anthropic API.
- `python-dotenv`: For loading environment variables from a `.env` file using `load_dotenv`.

### Internal:
- Internal cost calculation functions (no external dependencies required)

## Implementation Instructions

1.  **Structure:** Create a single public asynchronous function `call_claude_api`. Do not use a class for this module to keep it simple and functional.

2.  **API Client Initialization:**
    - At the top of the module, initialize the `anthropic.AsyncAnthropic` client.
    - The client should be initialized once and reused for all calls within the module's scope.
    - Call `load_dotenv(override=False)` before reading credentials, then read `ANTHROPIC_API_KEY` from the environment during initialization.

3.  **`call_claude_api` Function:**
    - Define an asynchronous function that accepts the following parameters:
      - A list of message dictionaries representing the conversation
      - A list of tool definitions for the API call
      - The model string identifier
      - An optional system prompt string
      - a boolean flag for cache usage
      - Optional parameters for max tokens (with reasonable default) and temperature (with conservative default)
    - The function should return a tuple containing the complete API response message object and the calculated cost as a float
    - Use proper type hints throughout, importing necessary types from the anthropic library and standard typing module

4.  **Request Payload Construction:**
    - Inside `call_claude_api`, prepare the arguments for the `client.messages.create` call.
    - **Caching Logic:**
        - If `use_cache` is `True`, identify the message containing the file content (it will typically be the first user message after the initial system prompt setup).
        - Reconstruct the `messages` list to use `cache_control`. The file content should be in a separate message block with `{"role": "user", "content": [{"type": "text", "text": file_content, "cache_control": {"type": "ephemeral"}}]}`. The user's edit instruction should be in a subsequent message block without `cache_control`.
    - **Standard Logic:** If `use_cache` is `False`, pass the `messages` list as is.

% Here is an example of a smart cache strategy: <caching_strategy_example><include>examples/utils_example.py</include></caching_strategy_example>

5.  **API Call and Retries:**
    - Wrap the API client call in appropriate exception handling.
    - Implement a retry loop with a reasonable number of attempts and exponential backoff timing for transient errors like rate limiting and connection issues.
    - For non-retryable errors or when retries are exhausted, re-raise the exception to be handled by the caller.

% Here is an example of a retry decorator for handling transient API errors: <retry_decorator_example><include>examples/utils_example.py</include></retry_decorator_example>

6.  **Cost Calculation:**
    - On a successful response, extract the `usage` object.
    - **IMPORTANT:** Use `input_tokens` directly as the standard/base tokens - do NOT subtract cache tokens from it.
    - Implement internal cost calculation logic using the token counts (`input_tokens`, `output_tokens`, `cache_creation_input_tokens`, `cache_read_input_tokens`) and current Anthropic pricing.
    - **Calculation:** `cost = (input_tokens/1M * input_rate) + (output_tokens/1M * output_rate) + (cache_creation_tokens/1M * input_rate) + (cache_read_tokens/1M * cache_read_rate)`
    - **NO negative token handling needed** - since input_tokens is already the correct base value, no subtraction should occur.
    - Create a helper function to calculate costs that accepts the model identifier and all token count parameters, returning the total cost and handling the logic of cache writes, reads, and standard calls.
    - **Maintain backward compatibility:** Return the same CostInfo structure with identical field names so dependent modules continue working unchanged.
    - **Interface Preservation**: Ensure field names in CostInfo match what dependent modules expect (e.g., `cache_creation_tokens`, `cache_read_tokens`).

% Here is an example of how to calculate API costs for different models and cache statuses: <cost_calculation_example><include>examples/utils_example.py</include></cost_calculation_example>

7.  **Return Value:**
    - Return the complete `response` object from the API call and the calculated `cost`. The calling function will be responsible for parsing the content from the response object.

## Key Functions and Classes

-   **Primary API Function**: The main asynchronous function that orchestrates API calls, handles caching, and calculates costs. Should accept all necessary parameters for API communication and return both the response and cost information.
-   **Cost Calculation Helper**: Internal helper function for cost calculation that maps model types to current Anthropic pricing and handles different token usage scenarios.

## Backward Compatibility Requirements

1. **Function Signatures**: All existing function signatures must remain identical to prevent breaking changes
2. **Return Types**: All return structures (especially CostInfo) must have same field names and types
3. **Module Exports**: All imports from dependent modules (cli.py, core.py) must continue working without modification
4. **Constants**: All exported constants must retain same names, types, and expected structure
5. **No Breaking Changes**: Any modification must not require changes to dependent modules

## Critical Implementation Notes

1. **Token Calculation:** Do NOT implement any logic to subtract cache tokens from input_tokens. Use input_tokens directly as the standard tokens.
2. **No Negative Token Warnings:** Do not add warning logs about negative tokens or clamping logic - these are unnecessary with correct token interpretation.
3. **Interface Compatibility:** Maintain the exact same function signatures and return types as the current implementation to ensure backward compatibility.
4. **CostInfo Structure:** Return dictionary must contain the same fields: `model`, `input_tokens`, `output_tokens`, `cache_creation_tokens`, `cache_read_tokens`, `total_cost`.
5. **MODEL_TO_TEXT_EDITOR_TOOL Structure:** Must be a dictionary that supports `{v['name'] for v in MODEL_TO_TEXT_EDITOR_TOOL.values()}` pattern used by core.py.

## Implementation Validation

Before completing implementation, verify:
1. All imports in cli.py and core.py work without modification: `DEFAULT_MODEL`, `SUPPORTED_MODELS`, `MODEL_TO_TEXT_EDITOR_TOOL`, `call_claude_api`
2. MODEL_TO_TEXT_EDITOR_TOOL structure supports the usage pattern: `{v['name'] for v in MODEL_TO_TEXT_EDITOR_TOOL.values()}`
3. CostInfo field names match dependent module expectations
4. Function signatures are identical to current implementation
5. All expected module exports are available at module level

## Usage Examples from Dependent Modules

Expected usage patterns that the implementation must support:

**CLI Module Usage:**
```python
# Model resolution and validation
from edit_file_tool.claude_api import DEFAULT_MODEL, SUPPORTED_MODELS

resolved_model = model_override or os.getenv("EDIT_FILE_TOOL_MODEL") or DEFAULT_MODEL
if resolved_model not in SUPPORTED_MODELS:
    raise ValueError(f"Model '{resolved_model}' is not supported.")
```

**Core Module Usage:**
```python
# Tool routing and API calls
from edit_file_tool.claude_api import MODEL_TO_TEXT_EDITOR_TOOL, call_claude_api

# Determine valid editor tool names for routing
editor_tool_names = {v['name'] for v in MODEL_TO_TEXT_EDITOR_TOOL.values()}

# Make API call
response, cost_info = await call_claude_api(
    messages=messages,
    model=model,
    system_prompt=SYSTEM_PROMPT,
    use_cache=should_cache,
)
total_cost += cost_info["total_cost"]
```

## Testing Requirements

1. **Compatibility Tests**: All existing imports from cli.py and core.py must work without modification
2. **Cost Calculation Tests**: Must pass with new token logic (no negative values)
3. **Module Structure Tests**: Verify all expected exports exist and have correct types
4. **Integration Tests**: Validate that dependent modules can import and use all required items
5. **Interface Tests**: Ensure MODEL_TO_TEXT_EDITOR_TOOL supports the expected iteration pattern

## Deliverables
1.  A complete and functional `edit_file_tool/claude_api.py` module that adheres to all requirements and implementation instructions.
2.  Full implementation of the `call_claude_api` function, including logic for prompt caching and error handling with retries.
3.  All required module exports (DEFAULT_MODEL, SUPPORTED_MODELS, MODEL_TO_TEXT_EDITOR_TOOL, call_claude_api, CostInfo) properly defined.
4.  Comprehensive Google-style docstrings and full `typing` annotations for the module and its functions.
5.  Backward compatibility verification - all dependent module imports work unchanged.
6.  Unit tests for `claude_api.py`. Use `unittest.mock` to patch `anthropic.AsyncAnthropic` and simulate various API responses:
    - A successful response without caching.
    - A successful response with a cache write (`cache_creation_input_tokens` > 0).
    - A successful response with a cache read (`cache_read_input_tokens` > 0).
    - An API error response (e.g., `anthropic.APIError`).
    - A rate limit error to test the retry mechanism.
7.  The code must be production-ready, well-tested, and follow Python best practices (PEP 8).